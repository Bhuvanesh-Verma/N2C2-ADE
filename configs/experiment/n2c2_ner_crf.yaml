# @package _global_

# to execute this experiment run:
# python train.py experiment=lisa-ner

defaults:
  - override /dataset: n2c2_prepared.yaml
  - override /datamodule: default.yaml
  - override /taskmodule: transformer_token_classification.yaml
  - override /model: token_classification_with_seq2seq_encoder_and_crf.yaml
  - override /callbacks: default.yaml
  - override /logger: wandb.yaml # TODO: install wandb and re-add
  - override /trainer: default.yaml

# all parameters below will be merged with parameters from default configurations set above
# this allows you to overwrite only specified parameters

# name of the run determines folder name in logs
name: "dataset+n2c2/task+entities"

tags: ["dataset=n2c2", "model=named_entity_recognition"]

seed: 777

transformer_model: FacebookAI/xlm-roberta-base

trainer:
  min_epochs: 5
  max_epochs: 20
  # gradient_clip_val: 0.5

datamodule:
  batch_size: 4

taskmodule:
  tokenizer_name_or_path: ${transformer_model}
  # define how to split long sequences into windows of max_length
  tokenize_kwargs:
    max_length: 512
    stride: 64
    truncation: True
    return_overflowing_tokens: True

model:
  model_name_or_path: ${transformer_model}
  # configure learning rate. Default learning rate is set in model constructor (here: TransformerTokenClassificationModel)
  learning_rate: 1e-5
  classifier_dropout: 0.6
  seq2seq_encoder:
    type: lstm
    bidirectional: True
    num_layers: 2
    hidden_size: 512

model_save_dir: ${paths.save_dir}/models/${name}/${now:%Y-%m-%d_%H-%M-%S}